{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 몽고DB 접속 정보로 크롤링된 데이터 불러오기\n",
    "mongo = MongoClient(cfg.mongo_DB_HOST, cfg.mongo_DB_PORT)\n",
    "news = mongo.myarticles.articles\n",
    "\n",
    "result = []\n",
    "for n in news.find():\n",
    "    result.append({ 'id' : n['_id'],\n",
    "                    'title': n['title'],\n",
    "                    'content' : n['content']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(369, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>내달 12일 코엑스서 기업 초청 사업화 설명회(대전=연합뉴스) 박주영 기자 = 음주...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>음주측정기처럼 '훅' 불어 질병 진단…카이스트 10대 기술 선정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>[CBS노컷뉴스 이재웅 기자](사진=자료사진)최근 엔씨소프트의 모바일 게임 리니지M...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>'리니지M' 환불거부 배짱영업 주의보</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>지난주 2천여명이 참석했던 파이썬 컨퍼런스 ‘파이콘’이 성황리에 끝났다. 60여개의...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>'얼리또라이’의 데이터 공부 도전기</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>도메인등록 전문업체 고대디(GoDaddy) CEO 블레이크 어빙이 은퇴를 선언했다....</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>블레이크 어빙 고대디 CEO 은퇴, 후임자는 스캇 와그너</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>마이크로스파인의 그립식 발 탑재(지디넷코리아=이정현 기자)넓고 평평한 지표면이 아닌...</td>\n",
       "      <td>http://news.naver.com/main/read.nhn?mode=LSD&amp;m...</td>\n",
       "      <td>곤충처럼 벽에 달라붙어 착륙하는 드론</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               content  \\\n",
       "364  내달 12일 코엑스서 기업 초청 사업화 설명회(대전=연합뉴스) 박주영 기자 = 음주...   \n",
       "365  [CBS노컷뉴스 이재웅 기자](사진=자료사진)최근 엔씨소프트의 모바일 게임 리니지M...   \n",
       "366  지난주 2천여명이 참석했던 파이썬 컨퍼런스 ‘파이콘’이 성황리에 끝났다. 60여개의...   \n",
       "367  도메인등록 전문업체 고대디(GoDaddy) CEO 블레이크 어빙이 은퇴를 선언했다....   \n",
       "368  마이크로스파인의 그립식 발 탑재(지디넷코리아=이정현 기자)넓고 평평한 지표면이 아닌...   \n",
       "\n",
       "                                                    id  \\\n",
       "364  http://news.naver.com/main/read.nhn?mode=LSD&m...   \n",
       "365  http://news.naver.com/main/read.nhn?mode=LSD&m...   \n",
       "366  http://news.naver.com/main/read.nhn?mode=LSD&m...   \n",
       "367  http://news.naver.com/main/read.nhn?mode=LSD&m...   \n",
       "368  http://news.naver.com/main/read.nhn?mode=LSD&m...   \n",
       "\n",
       "                                   title  \n",
       "364  음주측정기처럼 '훅' 불어 질병 진단…카이스트 10대 기술 선정  \n",
       "365                 '리니지M' 환불거부 배짱영업 주의보  \n",
       "366                  '얼리또라이’의 데이터 공부 도전기  \n",
       "367      블레이크 어빙 고대디 CEO 은퇴, 후임자는 스캇 와그너  \n",
       "368                 곤충처럼 벽에 달라붙어 착륙하는 드론  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 불러온 데이터 중에서 기사 본문을 읽어서 텍스트 문서로 저장하기\n",
    "USE_PREMADE_CONTENT = False\n",
    "\n",
    "from os import path\n",
    "content_filepath = 'task/content.txt'\n",
    "\n",
    "if not USE_PREMADE_CONTENT:\n",
    "    \n",
    "    with open(content_filepath, 'w', encoding='utf-8') as f:\n",
    "        for article in data.content.values:\n",
    "            # 기사가 없으면 스킵\n",
    "            if pd.isnull(article):\n",
    "                continue\n",
    "            f.write(article + '\\n')\n",
    "else:\n",
    "    assert path.exists(content_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 저장된 텍스트 문서에서 기사 읽어오는 함수\n",
    "def read_article(filepath):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for article in f:\n",
    "            yield article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 읽어온 모든 기사를 기사별로 나눠주는 함수. 숫자로 기사 불러오기.\n",
    "from itertools import islice\n",
    "\n",
    "def retrieve_article(sample_num):\n",
    "    return next(islice(read_article(content_filepath), sample_num, sample_num+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 한글 형태소 분석기. konlpy Twitter 사용\n",
    "from konlpy.tag import Twitter\n",
    "tw = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stop_words : 걸러내야 하는 단어 모음.\n",
    "# stop_words = open('intermediate/stop_words.txt').read().split('\\n')\n",
    "stop_words = []\n",
    "\n",
    "# 텍스트 정규화 : 여기서는 명사만 추출하고 추가적으로 정규화할 때 stop_words를 사용\n",
    "def normalize(text):\n",
    "    nouns = tw.nouns(text)\n",
    "    filtered_nouns = [ noun for noun in nouns if len(noun) > 1 and noun not in stop_words]\n",
    "    return filtered_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 저장된 기사의 텍스트를 정규화하여 저장하기\n",
    "USE_PREMADE_SENTENCES_NORMALIZED = False\n",
    "\n",
    "sentences_normalized_filepath = 'task/contents_normalized.txt'\n",
    "\n",
    "if not USE_PREMADE_SENTENCES_NORMALIZED:\n",
    "    \n",
    "    with open(sentences_normalized_filepath, 'w',  encoding='utf-8') as f:\n",
    "        for article_parsed in read_article(content_filepath):\n",
    "            sentence_parsed = normalize(article_parsed)\n",
    "            f.write(' '.join(sentence_parsed) + '\\n')\n",
    "            \n",
    "else:\n",
    "    assert path.exists(sentences_normalized_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# gensim 라이브러리에서 Phrases 클래스 로딩\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# 텍스트파일에서 line 단위로 iterate 하는 LineSentence 클래스 제공\n",
    "# 한번에 하나의 line을 출력(generator)\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 279 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "USE_PREMADE_BIGRAM_MODEL = False\n",
    "\n",
    "bigram_model_filepath = 'task/bigram_model'\n",
    "\n",
    "# 정규화된 텍스트를 input으로 받아서 해당 객체 형태로 출력한다.\n",
    "unigram_sentences = LineSentence(sentences_normalized_filepath)\n",
    "\n",
    "if not USE_PREMADE_BIGRAM_MODEL:\n",
    "    \n",
    "    # 출력된 객체 Phrases 클래스에 통과시켜서 bigram model 생성.\n",
    "    # bigram은 같이 등장하는 두개의 단어를 하나의 어구로 보게 하기 위함\n",
    "    bigram_model = Phrases(unigram_sentences)\n",
    "    bigram_model.save(bigram_model_filepath)\n",
    "    \n",
    "else:\n",
    "    bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python36\\lib\\site-packages\\gensim\\models\\phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 351 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 생성된 bigram model에 Linesentence을 통과 시킨 객채를 넣어 bigram 텍스트 생성\n",
    "USE_PREMADE_BIGRAM_SENTENCES = False\n",
    "\n",
    "bigram_sentences_filepath = 'task/bigram_sentences.txt'\n",
    "\n",
    "if not USE_PREMADE_BIGRAM_SENTENCES:\n",
    "    \n",
    "    with open(bigram_sentences_filepath, 'w', encoding='utf-8') as f:\n",
    "        for unigram_sentence in unigram_sentences:\n",
    "            bigram_sentence = bigram_model[unigram_sentence]\n",
    "            f.write(' '.join(bigram_sentence) + '\\n')\n",
    "else:\n",
    "    assert path.exists(bigram_sentences_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 272 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# n-gram을 한번 더 통과시켜서 3개 어구짜리 모델 생성\n",
    "USE_PREMADE_TRIGRAM_MODEL = False\n",
    "\n",
    "trigram_model_filepath = 'task/trigram_model'\n",
    "\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import Phrases\n",
    "\n",
    "if not USE_PREMADE_TRIGRAM_MODEL:\n",
    "    \n",
    "    bigram_sentences = LineSentence(bigram_sentences_filepath)\n",
    "    trigram_model = Phrases(bigram_sentences)\n",
    "    trigram_model.save(trigram_model_filepath)\n",
    "\n",
    "else:\n",
    "    trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python36\\lib\\site-packages\\gensim\\models\\phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 3개 어구까지 만들고 텍스트 파일로 저장. lda 모델에 사용하기 위한 마지막 단계\n",
    "USE_PREMADE_CONTENTS_FOR_LDA = False\n",
    "\n",
    "articles_for_lda_filepath = 'task/contents_for_lda.txt'\n",
    "\n",
    "if not USE_PREMADE_CONTENTS_FOR_LDA:\n",
    "    \n",
    "    with open(articles_for_lda_filepath, 'w', encoding='utf-8') as f:\n",
    "        \n",
    "        for article_parsed in read_article(content_filepath):\n",
    "            \n",
    "            unigram_article = normalize(article_parsed)\n",
    "            bigram_article = bigram_model[unigram_article]\n",
    "            trigram_article = trigram_model[bigram_article]\n",
    "            f.write(' '.join(trigram_article) + '\\n')\n",
    "else:\n",
    "    assert path.exists(contents_for_lda_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dictionary와 Corpus 클래스 로딩\n",
    "# Dictionary는 특정 단어에 id를 부여하기 위해 사용\n",
    "# Corpus는 말뭉치로 어느 단어가 몇회 등장하나\n",
    "from gensim.corpora import Dictionary, MmCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 가지고 있는 텍스트로 dictionary 생성\n",
    "USE_PREMADE_DICTIONARY = False\n",
    "\n",
    "dictionary_filepath = 'task/dictionary.dict'\n",
    "\n",
    "if not USE_PREMADE_DICTIONARY:\n",
    "    \n",
    "    articles_for_lda = LineSentence(articles_for_lda_filepath)\n",
    "    dictionary = Dictionary(articles_for_lda)\n",
    "    #dictionary.filter_extremes(no_below=3, no_above=0.3)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    dictionary.save(dictionary_filepath)\n",
    "else:\n",
    "    dictionary = Dictionary.load(dictionary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Corpus 생성\n",
    "USE_PREMADE_CORPUS = False\n",
    "\n",
    "corpus_filepath = 'task/corpus.mm'\n",
    "\n",
    "if not USE_PREMADE_CORPUS:\n",
    "    \n",
    "    def make_bow_corpus(filepath):\n",
    "        # 정규화된 기사를 읽어와서 bag of words로 만드는 함수.\n",
    "        # bag of words : 순서나 문맥과 관련 없는 단어의 집합\n",
    "        for article in LineSentence(filepath):\n",
    "            yield dictionary.doc2bow(article)\n",
    "            \n",
    "    MmCorpus.serialize(corpus_filepath, make_bow_corpus(articles_for_lda_filepath))\n",
    "    \n",
    "article_corpus = MmCorpus(corpus_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA 모델 로딩\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# corpus와 dictionary를 넣고, cpu 개수에 따라 worker 개수 설정하여 병렬처리,\n",
    "# num_topics 는 주제의 개수 설정, passes 는 전체 단어 학습 횟수\n",
    "# 위 인자 등을 가지고 lda 모델 생성\n",
    "USE_PREMADE_LDA = False\n",
    "\n",
    "lda_filepath = 'task/lda'\n",
    "\n",
    "if not USE_PREMADE_LDA:\n",
    "    \n",
    "    lda = LdaMulticore(article_corpus,\n",
    "                           num_topics=110,\n",
    "                           id2word=dictionary,\n",
    "                           workers=3,\n",
    "                           passes=200)\n",
    "    lda.save(lda_filepath)\n",
    "else:\n",
    "    lda = LdaMulticore.load(lda_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# USE_PREMADE_LDAVIS = False\n",
    "\n",
    "# ldavis_filepath = 'medium/ldavis'\n",
    "\n",
    "# if not USE_PREMADE_LDAVIS:\n",
    "#     ldavis = pyLDAvis.gensim.prepare(topic_model=lda, \n",
    "#                                      corpus=review_corpus, \n",
    "#                                      dictionary=dictionary)\n",
    "    \n",
    "#     with open(ldavis_filepath, 'wb') as f:\n",
    "#         pickle.dump(ldavis, f)\n",
    "\n",
    "# else:\n",
    "#     with open(ldavis_filepath, 'rb') as f:\n",
    "#         ldavis = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pyLDAvis.display(ldavis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 생성된 lda를 가지고 각 기사에 대한 topic을 추출하는 함수. 선택한 topic 개수에 따라 번호로 주제명이 부여됨.\n",
    "def get_article_lda(article):\n",
    "    article_lemmatized = normalize(article)\n",
    "    article_bigram = bigram_model[article_lemmatized]\n",
    "    article_trigram = trigram_model[article_bigram]\n",
    "    article_bow = dictionary.doc2bow(article_trigram)\n",
    "    article_topic = lda[article_bow]\n",
    "    \n",
    "    return article_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get_article_lda 로 뽑힌 주제중 각 주제에서 자주 등장하는 단어중 빈도가 제일 높은 단어 선택 -> 해당 주제의 주제로 선정하기 위해 사용\n",
    "def inner_topic(topic_num):\n",
    "    return sorted(lda.show_topic(topic_num), key=lambda x:x[1], reverse=True)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 주제 번호로 해당 주제를 나타내는 단어 찾기\n",
    "def get_topic_name(topic):\n",
    "    result = []\n",
    "    for x in topic:\n",
    "        result.append((x[0], inner_topic(x[0])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 각 기사의 번호를 가지고 해당 기사의 url과 제목, 주제 가지고 오는 함수\n",
    "# '' 값으로 처리된 rel1~7은 차후에 보다 검색 결과의 정확도를 높이기 위해서 미리 만들어 놓은 변수.\n",
    "def assign_topic2data(article_num):\n",
    "    single_article = retrieve_article(article_num)\n",
    "    rel1 = ''\n",
    "    rel2 = ''\n",
    "    rel3 = ''\n",
    "    rel4 = ''\n",
    "    rel5 = ''\n",
    "    rel6 = ''\n",
    "    rel7 = ''\n",
    "    topic = get_article_lda(single_article)\n",
    "    return (data.iloc[article_num].id, data.iloc[article_num].title, get_topic_name(topic)[0][1], rel1, rel2, rel3, rel4, rel5, rel6, rel7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mssql db 저장 모듈 로딩\n",
    "from newsdao import NewsDAO\n",
    "newsdao = NewsDAO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python36\\lib\\site-packages\\gensim\\models\\phrases.py:274: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# assign_topic2data 함수 호출하여 결과 값을 받아서 db에 저장\n",
    "for i in range(len(data)):\n",
    "    url, title, topic, rel1, rel2, rel3, rel4, rel5, rel6, rel7 = assign_topic2data(i)\n",
    "    newsdao.save_news(url, title, topic, rel1, rel2, rel3, rel4, rel5, rel6, rel7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
